{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74345eca-bb44-4b8c-80fe-ae0c1c29f0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset loaded successfully.\n",
      "Columns in dataset: ['url', 'type']\n",
      "Extracting features from URLs...\n",
      "Epoch 1, Loss: 0.4473\n",
      "Epoch 11, Loss: 0.2543\n",
      "Epoch 21, Loss: 0.2295\n",
      "Epoch 31, Loss: 0.2599\n",
      "Epoch 41, Loss: 0.2664\n",
      "\n",
      "=== Model Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.90      0.98      0.94     85621\n",
      "  defacement       0.91      0.97      0.94     19292\n",
      "     malware       0.95      0.84      0.89      6504\n",
      "    phishing       0.86      0.48      0.62     18822\n",
      "\n",
      "    accuracy                           0.90    130239\n",
      "   macro avg       0.90      0.82      0.85    130239\n",
      "weighted avg       0.90      0.90      0.89    130239\n",
      "\n",
      "✅ Model weights saved as 'gpu_forest_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =====================\n",
    "# DATA PREPROCESSING\n",
    "# =====================\n",
    "def calculate_entropy(string):\n",
    "    \"\"\"Calculate Shannon entropy of URL string\"\"\"\n",
    "    prob = [float(string.count(c)) / len(string) for c in set(string)]\n",
    "    return -sum(p * np.log2(p) for p in prob)\n",
    "\n",
    "def extract_features(url):\n",
    "    \"\"\"Extract URL security features\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "    path = parsed.path\n",
    "\n",
    "    features = {\n",
    "        'url_length': len(url),\n",
    "        'domain_length': len(domain),\n",
    "        'num_digits': sum(c.isdigit() for c in url),\n",
    "        'special_chars': sum(url.count(c) for c in ['@', '?', '=', '.', '-', '_', '/']),\n",
    "        'has_https': 1 if parsed.scheme == 'https' else 0,\n",
    "        'num_subdomains': domain.count('.') - 1 if domain.count('.') > 1 else 0,\n",
    "        'path_length': len(path),\n",
    "        'num_params': path.count('?') + path.count('&'),\n",
    "        'has_port': 1 if ':' in domain else 0,\n",
    "        'is_ip': 1 if re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', domain) else 0,\n",
    "        'file_extension': 1 if '.' in path.split('/')[-1] else 0,\n",
    "        'entropy': calculate_entropy(url),\n",
    "        'num_redirects': url.count('//') - 1,\n",
    "        'has_phish_keywords': 1 if any(kw in url.lower() for kw in ['login', 'verify', 'secure', 'account']) else 0\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# =====================\n",
    "# LOAD AND PREPARE DATA\n",
    "# =====================\n",
    "try:\n",
    "    df = pd.read_csv(\"malicious_phish.csv\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File 'malicious_phish.csv' not found in current directory\")\n",
    "    exit()\n",
    "\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "url_column = 'url'\n",
    "label_column = 'type'\n",
    "\n",
    "print(\"Extracting features from URLs...\")\n",
    "features = df[url_column].apply(extract_features).apply(pd.Series)\n",
    "X = features.values.astype(np.float32)\n",
    "y = df[label_column].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "y_train_tensor = torch.tensor(y_train).long().to(device)\n",
    "X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# =====================\n",
    "# MODEL DEFINITION\n",
    "# =====================\n",
    "class ForestNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_trees=300):\n",
    "        super().__init__()\n",
    "        self.trees = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, num_classes)\n",
    "            ) for _ in range(num_trees)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [tree(x) for tree in self.trees]\n",
    "        return torch.stack(outputs).mean(dim=0)\n",
    "\n",
    "model = ForestNet(\n",
    "    input_size=X_train.shape[1],\n",
    "    num_classes=len(le.classes_),\n",
    "    num_trees=300\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# =====================\n",
    "# TRAINING LOOP\n",
    "# =====================\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# =====================\n",
    "# EVALUATION\n",
    "# =====================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    y_pred = test_outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "print(\"\\n=== Model Evaluation ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# =====================\n",
    "# SAVE MODEL (weights only)\n",
    "# =====================\n",
    "torch.save(model.state_dict(), 'gpu_forest_model.pth')\n",
    "print(\"✅ Model weights saved as 'gpu_forest_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70fd40-a107-457d-ac13-244c69f0479f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40034f1-aa6a-4ea8-a3c6-288a3de54a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU (cu121)",
   "language": "python",
   "name": "torch-cu121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
